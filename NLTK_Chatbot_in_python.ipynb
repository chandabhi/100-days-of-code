{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzgZB+x54pw5e/AjIgwKCI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandabhi/100-days-of-code/blob/master/NLTK_Chatbot_in_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "mnXJpLyna2p-"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/data.txt','r',errors = 'ignore')\n",
        "raw_doc = f.read()"
      ],
      "metadata": {
        "id": "95d38HHlhhzH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_KZGqXO5o1Pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "raw_doc =raw_doc.lower()\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "sentence_tokens = nltk.sent_tokenize(raw_doc)\n",
        "word_tokens = nltk.word_tokenize(raw_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8WCK69UoHuh",
        "outputId": "ee35843c-d0b1-4713-8167-621860ea2ea2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "btulOvbIiRZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens[:5]"
      ],
      "metadata": {
        "id": "be0Ggd-Wc-zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7-pV2ZpgTgS",
        "outputId": "3cafa463-897d-4c64-dabb-74889f9a38a3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['in', '1950', ',', 'alan', 'turing']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer  = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punc_dict = dict((ord(punct),None)for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))\n"
      ],
      "metadata": {
        "id": "kRbnsYIjjQFd"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Greeting Functions"
      ],
      "metadata": {
        "id": "W9RCXnD7k16E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greet_inputs = ('hello','hi','whassup','how are you?')\n",
        "greet_responses = ('hi','hey','Hey there!')\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greet_inputs:\n",
        "      return random.choice(greet_responses)"
      ],
      "metadata": {
        "id": "AWaZHQaHk2ra"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-f8C3IbCwneF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response Generation by the Bot"
      ],
      "metadata": {
        "id": "azOj_w05lzig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer #Models vcan take word directly to train , so converted them into numbers\n",
        "from sklearn.metrics.pairwise import cosine_similarity #Used to check similarity between two sentences/vectors"
      ],
      "metadata": {
        "id": "SpwuyijEmLaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_Response):\n",
        "  robo1_response = ''\n",
        "  TfidfVec = TfidfVectorizer(tokenizer = LemNormalize , stop_words = 'english')\n",
        "  tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if(req_tfidf == 0):\n",
        "    robo1_response = robo1_response + \"I am sorry. Unable to understad you!\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response =  robo1_response + sentence_tokens[idx]\n",
        "    return robo1_response\n",
        ""
      ],
      "metadata": {
        "id": "LKXN3lDvngrl"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a chat flow"
      ],
      "metadata": {
        "id": "CNmCXWGOtJYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import nltk\n",
        "# import string\n",
        "# import random\n",
        "\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer #Models vcan take word directly to train , so converted them into numbers\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# def response(user_Response):\n",
        "#   robo1_response = ''\n",
        "#   TfidfVec = TfidfVectorizer(tokenizer = LenNormalize , stop_words = 'english')\n",
        "#   tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
        "#   vals = cosine_similiarity(tfid[-1],tfidf)\n",
        "#   idx = vals.argsort()[0][-2]\n",
        "#   flat = vals.flatten()\n",
        "#   flat.sort()\n",
        "#   req_tfidf = flat[-2]\n",
        "#   if(req_tfidf == 0):\n",
        "#     robo1_response = robop1_response + \"I am sorry. Unable to understad you!\"\n",
        "#     return robop1_response\n",
        "#   else:\n",
        "#     robo1_response =  robo1_response + sentence_tokens[jdx]\n",
        "#     return robo1_response\n",
        "\n",
        "flag = True\n",
        "print('Hello! I am the retreival Learning Bot .Start typing your text after greeting to talk to me. For ending convo type bye!')\n",
        "while(flag == True):\n",
        "  user_response = input()\n",
        "  user_Response = user_response.lower()\n",
        "  if(user_response != 'bye'):\n",
        "    if(user_response == 'thank you' or user_response == 'thanks'):\n",
        "      flag = False\n",
        "      print('Bot: You are welcome..')\n",
        "    else:\n",
        "      if(greet(user_response)!= None):\n",
        "        print('Bot '+ greet(user_response))\n",
        "      else:\n",
        "        sentence_tokens.append(user_response)\n",
        "        word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
        "        final_words = list(set(word_tokens))\n",
        "        print('Bot: ', end = '')\n",
        "        print(response(user_response))\n",
        "        sentence_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag = False\n",
        "    print('Bot: Goodbye!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3pJBlVHtL65",
        "outputId": "f6cdfbe2-7987-4877-d489-f8ecc3417f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I am the retreival Learning Bot .Start typing your text after greeting to talk to me. For ending convo type bye!\n",
            "hi\n",
            "Bot hey\n",
            "hlo\n",
            "Bot: hlo\n",
            "hy\n",
            "Bot: I am sorry. Unable to understad you!\n",
            "hlo\n",
            "Bot: hlo\n",
            "hello\n",
            "Bot Hey there!\n",
            "hey\n",
            "Bot: I am sorry. Unable to understad you!\n",
            "ho\n",
            "Bot: I am sorry. Unable to understad you!\n",
            "hijknc\n",
            "Bot: I am sorry. Unable to understad you!\n",
            "hbcjbdcj\n",
            "Bot: I am sorry. Unable to understad you!\n",
            "explained\n",
            "Bot: the object of this paper is to cause just such a re-evaluation of the program about to be \n",
            "\"explained\".\n",
            "ELIZA\n",
            "Bot: eliza showed that such an illusion is surprisingly \n",
            "easy to generate because human judges are so ready to give the benefit of the doubt when conversational responses are capable of being \n",
            "interpreted as \"intelligent\".\n",
            "ELIZA\n",
            "Bot: eliza showed that such an illusion is surprisingly \n",
            "easy to generate because human judges are so ready to give the benefit of the doubt when conversational responses are capable of being \n",
            "interpreted as \"intelligent\".\n",
            "Computing\n",
            "Bot: in 1950, alan turing's famous article \"computing machinery and intelligence\" was published,[12] which proposed what is now called the turing\n",
            "test as a criterion of intelligence.\n",
            "procedures\n",
            "Bot: but \n",
            "once a particular program is unmasked, once its inner workings are explained, its magic crumbles away; it stands revealed as a mere collection\n",
            "of procedures.\n",
            "judge\n",
            "Bot: this criterion depends on the ability of a computer program to impersonate a human in a real-time written\n",
            "conversation with a human judge to the extent that the judge is unable to distinguish reliably—on the basis of the conversational content \n",
            "alone—between the program and a real human.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vKRBpyp3Fu-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SiTBFw68uKyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cb2AhjNiwIcZ"
      }
    }
  ]
}